{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Model Training\n",
    "## MovieLens 32M Dataset (30% Sample)\n",
    "\n",
    "**Models to Train:**\n",
    "1. **Baseline Models** - Popularity, Random, Global Mean\n",
    "2. **Matrix Factorization** - Custom SVD (NumPy/SciPy)\n",
    "3. **Content-Based** - Genre similarity\n",
    "4. **Hybrid** - Weighted combination of MF + Content-Based\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- RMSE, MAE (rating prediction)\n",
    "- Precision@K, Recall@K, NDCG@K (ranking)\n",
    "- Coverage, Diversity (beyond accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded (no external dependencies needed).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from scipy.sparse import load_npz, csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries loaded (no external dependencies needed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION\n",
      "==================================================\n",
      "Data path: D:/Courses/DL INTERNSHIP/THIRD PROJECT/data/ml_ready\n",
      "Models path: D:/Courses/DL INTERNSHIP/THIRD PROJECT/models\n",
      "Top-K values: [5, 10, 20]\n",
      "Relevance threshold: 4.0\n",
      "SVD factors: 50\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CONFIGURATION\n",
    "# ===========================================\n",
    "\n",
    "ML_READY_PATH = 'data/ml_ready'\n",
    "MODELS_PATH = '/models'\n",
    "\n",
    "# Evaluation settings\n",
    "TOP_K = [5, 10, 20]  # For Precision@K, Recall@K, NDCG@K\n",
    "RELEVANCE_THRESHOLD = 4.0  # Rating >= this is \"relevant\"\n",
    "\n",
    "# SVD settings\n",
    "N_FACTORS = 50  # Number of latent factors (reduced for speed)\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Data path: {ML_READY_PATH}\")\n",
    "print(f\"Models path: {MODELS_PATH}\")\n",
    "print(f\"Top-K values: {TOP_K}\")\n",
    "print(f\"Relevance threshold: {RELEVANCE_THRESHOLD}\")\n",
    "print(f\"SVD factors: {N_FACTORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. LOAD DATA\n",
      "============================================================\n",
      "\n",
      "Train: 6,690,428 ratings\n",
      "Val: 1,437,861 ratings\n",
      "Test: 1,468,027 ratings\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. LOAD DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load splits\n",
    "train_df = pd.read_parquet(f'{ML_READY_PATH}/train.parquet')\n",
    "val_df = pd.read_parquet(f'{ML_READY_PATH}/val.parquet')\n",
    "test_df = pd.read_parquet(f'{ML_READY_PATH}/test.parquet')\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df):,} ratings\")\n",
    "print(f\"Val: {len(val_df):,} ratings\")\n",
    "print(f\"Test: {len(test_df):,} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparse matrix shape: (60284, 27498)\n",
      "Density: 0.4036%\n"
     ]
    }
   ],
   "source": [
    "# Load sparse matrices\n",
    "train_sparse = load_npz(f'{ML_READY_PATH}/train_sparse.npz')\n",
    "\n",
    "print(f\"\\nSparse matrix shape: {train_sparse.shape}\")\n",
    "print(f\"Density: {train_sparse.nnz / (train_sparse.shape[0] * train_sparse.shape[1]) * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users: 60,284\n",
      "Items: 27,498\n",
      "Global mean: 3.5377\n"
     ]
    }
   ],
   "source": [
    "# Load mappings and stats\n",
    "with open(f'{ML_READY_PATH}/mappings.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "with open(f'{ML_READY_PATH}/stats.pkl', 'rb') as f:\n",
    "    stats = pickle.load(f)\n",
    "\n",
    "with open(f'{ML_READY_PATH}/eval_data.pkl', 'rb') as f:\n",
    "    eval_data = pickle.load(f)\n",
    "\n",
    "n_users = mappings['n_users']\n",
    "n_items = mappings['n_items']\n",
    "global_mean = stats['global_mean']\n",
    "user_bias = stats['user_bias']\n",
    "item_bias = stats['item_bias']\n",
    "item_popularity = stats['item_popularity']\n",
    "\n",
    "print(f\"\\nUsers: {n_users:,}\")\n",
    "print(f\"Items: {n_items:,}\")\n",
    "print(f\"Global mean: {global_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genre features: (27498, 19)\n",
      "Genres: ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "# Load genre features\n",
    "genre_features = np.load(f'{ML_READY_PATH}/genre_features.npy')\n",
    "genre_names = mappings['genre_names']\n",
    "\n",
    "print(f\"\\nGenre features: {genre_features.shape}\")\n",
    "print(f\"Genres: {genre_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val users: 60,284\n",
      "Test users: 60,284\n"
     ]
    }
   ],
   "source": [
    "# Evaluation data\n",
    "val_user_items = eval_data['val_user_items']\n",
    "test_user_items = eval_data['test_user_items']\n",
    "val_relevant = eval_data['val_relevant']\n",
    "test_relevant = eval_data['test_relevant']\n",
    "user_positive_items = eval_data['user_positive_items']\n",
    "\n",
    "print(f\"\\nVal users: {len(val_user_items):,}\")\n",
    "print(f\"Test users: {len(test_user_items):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def rmse(predictions, actuals):\n",
    "    \"\"\"Root Mean Square Error\"\"\"\n",
    "    return np.sqrt(np.mean((np.array(predictions) - np.array(actuals)) ** 2))\n",
    "\n",
    "def mae(predictions, actuals):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(np.array(predictions) - np.array(actuals)))\n",
    "\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    \"\"\"Precision@K\"\"\"\n",
    "    if len(recommended) == 0:\n",
    "        return 0.0\n",
    "    recommended_k = set(recommended[:k])\n",
    "    return len(recommended_k & relevant) / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    \"\"\"Recall@K\"\"\"\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    recommended_k = set(recommended[:k])\n",
    "    return len(recommended_k & relevant) / len(relevant)\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended[:k]):\n",
    "        if item in relevant:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    # Ideal DCG\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def coverage(all_recommended, n_items):\n",
    "    \"\"\"Catalog coverage - % of items recommended at least once\"\"\"\n",
    "    unique_items = set()\n",
    "    for items in all_recommended:\n",
    "        unique_items.update(items)\n",
    "    return len(unique_items) / n_items * 100\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ranking(model_name, recommendations, relevant_items, ks=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Evaluate ranking metrics for a model.\n",
    "    \"\"\"\n",
    "    results = {'model': model_name}\n",
    "    \n",
    "    for k in ks:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        ndcgs = []\n",
    "        \n",
    "        for user_idx, recs in recommendations.items():\n",
    "            if user_idx in relevant_items and len(relevant_items[user_idx]) > 0:\n",
    "                rel = relevant_items[user_idx]\n",
    "                precisions.append(precision_at_k(recs, rel, k))\n",
    "                recalls.append(recall_at_k(recs, rel, k))\n",
    "                ndcgs.append(ndcg_at_k(recs, rel, k))\n",
    "        \n",
    "        results[f'P@{k}'] = np.mean(precisions) if precisions else 0.0\n",
    "        results[f'R@{k}'] = np.mean(recalls) if recalls else 0.0\n",
    "        results[f'NDCG@{k}'] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "    \n",
    "    # Coverage\n",
    "    results['Coverage'] = coverage(list(recommendations.values()), n_items)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Ranking evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Global Mean Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3.1 GLOBAL MEAN BASELINE\n",
      "============================================================\n",
      "\n",
      "Global Mean: 3.5377\n",
      "Val RMSE: 1.0595\n",
      "Val MAE: 0.8380\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3.1 GLOBAL MEAN BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Predict global mean for all\n",
    "val_predictions_mean = [global_mean] * len(val_df)\n",
    "val_actuals = val_df['rating'].values\n",
    "\n",
    "rmse_mean = rmse(val_predictions_mean, val_actuals)\n",
    "mae_mean = mae(val_predictions_mean, val_actuals)\n",
    "\n",
    "print(f\"\\nGlobal Mean: {global_mean:.4f}\")\n",
    "print(f\"Val RMSE: {rmse_mean:.4f}\")\n",
    "print(f\"Val MAE: {mae_mean:.4f}\")\n",
    "\n",
    "all_results.append({\n",
    "    'model': 'Global Mean',\n",
    "    'RMSE': rmse_mean,\n",
    "    'MAE': mae_mean,\n",
    "    'P@10': 0.0,\n",
    "    'R@10': 0.0,\n",
    "    'NDCG@10': 0.0,\n",
    "    'Coverage': 0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 User-Item Bias Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3.2 USER-ITEM BIAS BASELINE\n",
      "============================================================\n",
      "Predicting for validation set...\n",
      "\n",
      "Val RMSE: 0.8775\n",
      "Val MAE: 0.6657\n",
      "Improvement over Global Mean: 17.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3.2 USER-ITEM BIAS BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def predict_bias(user_idx, item_idx):\n",
    "    \"\"\"Predict: global_mean + user_bias + item_bias\"\"\"\n",
    "    u_bias = user_bias.get(user_idx, 0)\n",
    "    i_bias = item_bias.get(item_idx, 0)\n",
    "    pred = global_mean + u_bias + i_bias\n",
    "    return np.clip(pred, 0.5, 5.0)  # Clip to valid range\n",
    "\n",
    "# Predict for validation\n",
    "print(\"Predicting for validation set...\")\n",
    "val_predictions_bias = [\n",
    "    predict_bias(row['user_idx'], row['item_idx'])\n",
    "    for _, row in val_df.iterrows()\n",
    "]\n",
    "\n",
    "rmse_bias = rmse(val_predictions_bias, val_actuals)\n",
    "mae_bias = mae(val_predictions_bias, val_actuals)\n",
    "\n",
    "print(f\"\\nVal RMSE: {rmse_bias:.4f}\")\n",
    "print(f\"Val MAE: {mae_bias:.4f}\")\n",
    "print(f\"Improvement over Global Mean: {(rmse_mean - rmse_bias) / rmse_mean * 100:.1f}%\")\n",
    "\n",
    "all_results.append({\n",
    "    'model': 'User-Item Bias',\n",
    "    'RMSE': rmse_bias,\n",
    "    'MAE': mae_bias,\n",
    "    'P@10': 0.0,\n",
    "    'R@10': 0.0,\n",
    "    'NDCG@10': 0.0,\n",
    "    'Coverage': 0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Popularity Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3.3 POPULARITY BASELINE\n",
      "============================================================\n",
      "\n",
      "Top 10 most popular items: [314, 351, 292, 2429, 582, 257, 2812, 472, 519, 4785]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3.3 POPULARITY BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Rank items by popularity (number of ratings)\n",
    "popularity_ranking = sorted(item_popularity.items(), key=lambda x: x[1], reverse=True)\n",
    "popular_items = [item for item, _ in popularity_ranking]\n",
    "\n",
    "print(f\"\\nTop 10 most popular items: {popular_items[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating popularity recommendations...\n",
      "\n",
      "Popularity Baseline Results:\n",
      "  P@5: 0.0967\n",
      "  R@5: 0.0574\n",
      "  NDCG@5: 0.1112\n",
      "  P@10: 0.0819\n",
      "  R@10: 0.0933\n",
      "  NDCG@10: 0.1136\n",
      "  P@20: 0.0655\n",
      "  R@20: 0.1461\n",
      "  NDCG@20: 0.1249\n",
      "  Coverage: 0.36%\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations (same for all users, excluding seen items)\n",
    "def get_popularity_recommendations(user_idx, popular_items, user_positive, k=20):\n",
    "    \"\"\"Recommend popular items user hasn't seen.\"\"\"\n",
    "    seen = user_positive.get(user_idx, set())\n",
    "    recs = [item for item in popular_items if item not in seen][:k]\n",
    "    return recs\n",
    "\n",
    "# Generate for all val users\n",
    "popularity_recs = {}\n",
    "for user_idx in val_user_items.keys():\n",
    "    popularity_recs[user_idx] = get_popularity_recommendations(\n",
    "        user_idx, popular_items, user_positive_items, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "# Evaluate\n",
    "popularity_results = evaluate_ranking('Popularity', popularity_recs, val_relevant, TOP_K)\n",
    "popularity_results['RMSE'] = '-'\n",
    "popularity_results['MAE'] = '-'\n",
    "\n",
    "print(f\"\\nPopularity Baseline Results:\")\n",
    "for k in TOP_K:\n",
    "    print(f\"  P@{k}: {popularity_results[f'P@{k}']:.4f}\")\n",
    "    print(f\"  R@{k}: {popularity_results[f'R@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {popularity_results[f'NDCG@{k}']:.4f}\")\n",
    "print(f\"  Coverage: {popularity_results['Coverage']:.2f}%\")\n",
    "\n",
    "all_results.append(popularity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Matrix Factorization (SVD)\n",
    "\n",
    "Using scipy's truncated SVD on the sparse rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. MATRIX FACTORIZATION (SVD)\n",
      "============================================================\n",
      "\n",
      "Preparing matrix for SVD...\n",
      "Matrix shape: (60284, 27498)\n",
      "Non-zero entries: 6,690,428\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4. MATRIX FACTORIZATION (SVD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to lil_matrix for efficient modification\n",
    "train_centered = train_sparse.copy().tolil()\n",
    "train_centered_csr = train_sparse.copy().tocsr()\n",
    "\n",
    "# Subtract global mean from non-zero entries\n",
    "train_centered_csr.data -= global_mean\n",
    "\n",
    "print(f\"Matrix shape: {train_centered_csr.shape}\")\n",
    "print(f\"Non-zero entries: {train_centered_csr.nnz:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing SVD with 50 factors...\n",
      "SVD completed in 2.0 seconds\n",
      "\n",
      "U shape: (60284, 50)\n",
      "Sigma: (50,)\n",
      "Vt shape: (50, 27498)\n"
     ]
    }
   ],
   "source": [
    "# Perform truncated SVD\n",
    "print(f\"\\nPerforming SVD with {N_FACTORS} factors...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# svds returns U, sigma, Vt\n",
    "U, sigma, Vt = svds(train_centered_csr.astype(np.float32), k=N_FACTORS)\n",
    "\n",
    "# Convert sigma to diagonal matrix for reconstruction\n",
    "sigma_diag = np.diag(sigma)\n",
    "\n",
    "svd_time = time.time() - start_time\n",
    "print(f\"SVD completed in {svd_time:.1f} seconds\")\n",
    "print(f\"\\nU shape: {U.shape}\")\n",
    "print(f\"Sigma: {sigma.shape}\")\n",
    "print(f\"Vt shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User factors: (60284, 50)\n",
      "Item factors: (27498, 50)\n"
     ]
    }
   ],
   "source": [
    "# Create user and item latent factors\n",
    "# User factors: U @ sqrt(sigma)\n",
    "# Item factors: sqrt(sigma) @ Vt\n",
    "sigma_sqrt = np.sqrt(sigma)\n",
    "\n",
    "user_factors = U * sigma_sqrt  # (n_users, n_factors)\n",
    "item_factors = (Vt.T * sigma_sqrt).T  # (n_factors, n_items) -> transpose to (n_items, n_factors)\n",
    "item_factors = item_factors.T\n",
    "\n",
    "print(f\"\\nUser factors: {user_factors.shape}\")\n",
    "print(f\"Item factors: {item_factors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD prediction function defined.\n"
     ]
    }
   ],
   "source": [
    "def predict_svd(user_idx, item_idx):\n",
    "    \"\"\"Predict rating using SVD factors + biases.\"\"\"\n",
    "    # Convert to int explicitly (fixes float index issue)\n",
    "    user_idx = int(user_idx)\n",
    "    item_idx = int(item_idx)\n",
    "    \n",
    "    # Base prediction from latent factors\n",
    "    if user_idx < len(user_factors) and item_idx < len(item_factors):\n",
    "        latent_pred = np.dot(user_factors[user_idx], item_factors[item_idx])\n",
    "    else:\n",
    "        latent_pred = 0\n",
    "    \n",
    "    # Add global mean and biases\n",
    "    u_bias = user_bias.get(user_idx, 0)\n",
    "    i_bias = item_bias.get(item_idx, 0)\n",
    "    \n",
    "    pred = global_mean + u_bias + i_bias + latent_pred\n",
    "    return np.clip(pred, 0.5, 5.0)\n",
    "\n",
    "print(\"SVD prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating rating prediction on validation set...\n",
      "  Progress: 0/1,437,861\n",
      "  Progress: 200,000/1,437,861\n",
      "  Progress: 400,000/1,437,861\n",
      "  Progress: 600,000/1,437,861\n",
      "  Progress: 800,000/1,437,861\n",
      "  Progress: 1,000,000/1,437,861\n",
      "  Progress: 1,200,000/1,437,861\n",
      "  Progress: 1,400,000/1,437,861\n",
      "\n",
      "SVD Results (Rating Prediction):\n",
      "  Val RMSE: 0.8707\n",
      "  Val MAE: 0.6568\n",
      "  Improvement over Bias: 0.8%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set - Rating Prediction\n",
    "print(\"\\nEvaluating rating prediction on validation set...\")\n",
    "\n",
    "val_predictions_svd = []\n",
    "for i, (_, row) in enumerate(val_df.iterrows()):\n",
    "    if i % 200000 == 0:\n",
    "        print(f\"  Progress: {i:,}/{len(val_df):,}\")\n",
    "    # Convert to int explicitly\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    pred = predict_svd(user_idx, item_idx)\n",
    "    val_predictions_svd.append(pred)\n",
    "\n",
    "rmse_svd = rmse(val_predictions_svd, val_actuals)\n",
    "mae_svd = mae(val_predictions_svd, val_actuals)\n",
    "\n",
    "print(f\"\\nSVD Results (Rating Prediction):\")\n",
    "print(f\"  Val RMSE: {rmse_svd:.4f}\")\n",
    "print(f\"  Val MAE: {mae_svd:.4f}\")\n",
    "print(f\"  Improvement over Bias: {(rmse_bias - rmse_svd) / rmse_bias * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precomputing predicted ratings matrix...\n",
      "Prediction matrix shape: (60284, 27498)\n",
      "Computation time: 18.0 seconds\n",
      "Memory: 6.18 GB\n"
     ]
    }
   ],
   "source": [
    "# Precompute all predicted ratings for faster recommendation\n",
    "print(\"\\nPrecomputing predicted ratings matrix...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute full prediction matrix: U @ sigma @ Vt + global_mean\n",
    "# This is memory intensive but much faster for recommendations\n",
    "predicted_ratings = np.dot(user_factors, item_factors.T)  # (n_users, n_items)\n",
    "\n",
    "# Add biases\n",
    "user_bias_array = np.array([user_bias.get(i, 0) for i in range(n_users)])\n",
    "item_bias_array = np.array([item_bias.get(i, 0) for i in range(n_items)])\n",
    "\n",
    "predicted_ratings += global_mean\n",
    "predicted_ratings += user_bias_array.reshape(-1, 1)\n",
    "predicted_ratings += item_bias_array.reshape(1, -1)\n",
    "\n",
    "# Clip\n",
    "predicted_ratings = np.clip(predicted_ratings, 0.5, 5.0)\n",
    "\n",
    "print(f\"Prediction matrix shape: {predicted_ratings.shape}\")\n",
    "print(f\"Computation time: {time.time() - start_time:.1f} seconds\")\n",
    "print(f\"Memory: {predicted_ratings.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating SVD recommendations...\n",
      "  Processing user 0/3000\n",
      "  Processing user 1000/3000\n",
      "  Processing user 2000/3000\n",
      "\n",
      "Generated recommendations for 3000 users\n"
     ]
    }
   ],
   "source": [
    "# Generate SVD recommendations\n",
    "print(\"\\nGenerating SVD recommendations...\")\n",
    "\n",
    "def get_svd_recommendations(user_idx, predicted_ratings, user_positive, k=20):\n",
    "    \"\"\"Get top-k recommendations for a user using precomputed SVD predictions.\"\"\"\n",
    "    seen = user_positive.get(user_idx, set())\n",
    "    \n",
    "    # Get predictions for this user\n",
    "    user_preds = predicted_ratings[user_idx].copy()\n",
    "    \n",
    "    # Set seen items to -inf so they won't be recommended\n",
    "    for item in seen:\n",
    "        if item < len(user_preds):\n",
    "            user_preds[item] = -np.inf\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k_items = np.argsort(user_preds)[-k:][::-1]\n",
    "    \n",
    "    return top_k_items.tolist()\n",
    "\n",
    "# Sample users for efficiency\n",
    "sample_users = list(val_relevant.keys())[:3000]  # Sample 3000 users\n",
    "\n",
    "svd_recs = {}\n",
    "for i, user_idx in enumerate(sample_users):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processing user {i}/{len(sample_users)}\")\n",
    "    svd_recs[user_idx] = get_svd_recommendations(\n",
    "        user_idx, predicted_ratings, user_positive_items, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "print(f\"\\nGenerated recommendations for {len(svd_recs)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVD Results (Ranking):\n",
      "  P@5: 0.0111\n",
      "  R@5: 0.0034\n",
      "  NDCG@5: 0.0128\n",
      "  P@10: 0.0121\n",
      "  R@10: 0.0073\n",
      "  NDCG@10: 0.0137\n",
      "  P@20: 0.0115\n",
      "  R@20: 0.0133\n",
      "  NDCG@20: 0.0150\n",
      "  Coverage: 6.77%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ranking\n",
    "svd_results = evaluate_ranking('SVD', svd_recs, val_relevant, TOP_K)\n",
    "svd_results['RMSE'] = rmse_svd\n",
    "svd_results['MAE'] = mae_svd\n",
    "\n",
    "print(f\"\\nSVD Results (Ranking):\")\n",
    "for k in TOP_K:\n",
    "    print(f\"  P@{k}: {svd_results[f'P@{k}']:.4f}\")\n",
    "    print(f\"  R@{k}: {svd_results[f'R@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {svd_results[f'NDCG@{k}']:.4f}\")\n",
    "print(f\"  Coverage: {svd_results['Coverage']:.2f}%\")\n",
    "\n",
    "all_results.append(svd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Content-Based Filtering (Genre Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5. CONTENT-BASED FILTERING\n",
      "============================================================\n",
      "\n",
      "Computing genre similarity matrix...\n",
      "Similarity matrix shape: (27498, 27498)\n",
      "Computation time: 3.1 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"5. CONTENT-BASED FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute item-item similarity based on genres\n",
    "print(\"\\nComputing genre similarity matrix...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Normalize genre features (L2 norm)\n",
    "genre_norms = np.linalg.norm(genre_features, axis=1, keepdims=True)\n",
    "genre_norms[genre_norms == 0] = 1  # Avoid division by zero\n",
    "genre_features_normalized = genre_features / genre_norms\n",
    "\n",
    "# Compute cosine similarity\n",
    "item_similarity = cosine_similarity(genre_features_normalized)\n",
    "\n",
    "print(f\"Similarity matrix shape: {item_similarity.shape}\")\n",
    "print(f\"Computation time: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building user-item rating lookup...\n",
      "Built rating lookup for 60284 users\n"
     ]
    }
   ],
   "source": [
    "# Build user-item rating lookup from training data\n",
    "print(\"\\nBuilding user-item rating lookup...\")\n",
    "user_item_ratings = defaultdict(dict)\n",
    "for _, row in train_df.iterrows():\n",
    "    user_item_ratings[row['user_idx']][row['item_idx']] = row['rating']\n",
    "\n",
    "print(f\"Built rating lookup for {len(user_item_ratings)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-based recommendation function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_content_recommendations(user_idx, user_positive, item_similarity, user_item_ratings, k=20):\n",
    "    \"\"\"\n",
    "    Content-based recommendations using genre similarity.\n",
    "    \"\"\"\n",
    "    seen = user_positive.get(user_idx, set())\n",
    "    if len(seen) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Get items the user liked (rating >= threshold)\n",
    "    liked_items = []\n",
    "    for item_idx in seen:\n",
    "        if item_idx in user_item_ratings.get(user_idx, {}):\n",
    "            if user_item_ratings[user_idx][item_idx] >= RELEVANCE_THRESHOLD:\n",
    "                liked_items.append(item_idx)\n",
    "    \n",
    "    if len(liked_items) == 0:\n",
    "        liked_items = list(seen)[:10]  # Fall back to some seen items\n",
    "    \n",
    "    # Score unseen items by average similarity to liked items\n",
    "    n_items = item_similarity.shape[0]\n",
    "    scores = np.zeros(n_items)\n",
    "    \n",
    "    for liked in liked_items:\n",
    "        if liked < n_items:\n",
    "            scores += item_similarity[liked]\n",
    "    \n",
    "    scores /= len(liked_items)\n",
    "    \n",
    "    # Set seen items to -inf\n",
    "    for item in seen:\n",
    "        if item < n_items:\n",
    "            scores[item] = -np.inf\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k = np.argsort(scores)[-k:][::-1]\n",
    "    \n",
    "    return top_k.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating content-based recommendations...\n",
      "  Processing user 0/3000\n",
      "  Processing user 1000/3000\n",
      "  Processing user 2000/3000\n",
      "\n",
      "Generated recommendations for 3000 users\n"
     ]
    }
   ],
   "source": [
    "cb_recs = {}\n",
    "for i, user_idx in enumerate(sample_users):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processing user {i}/{len(sample_users)}\")\n",
    "    cb_recs[user_idx] = get_content_recommendations(\n",
    "        user_idx, user_positive_items, item_similarity, user_item_ratings, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "print(f\"\\nGenerated recommendations for {len(cb_recs)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content-Based Results:\n",
      "  P@5: 0.0017\n",
      "  R@5: 0.0008\n",
      "  NDCG@5: 0.0017\n",
      "  P@10: 0.0022\n",
      "  R@10: 0.0028\n",
      "  NDCG@10: 0.0025\n",
      "  P@20: 0.0022\n",
      "  R@20: 0.0053\n",
      "  NDCG@20: 0.0035\n",
      "  Coverage: 35.75%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "cb_results = evaluate_ranking('Content-Based', cb_recs, val_relevant, TOP_K)\n",
    "cb_results['RMSE'] = '-'\n",
    "cb_results['MAE'] = '-'\n",
    "\n",
    "print(f\"\\nContent-Based Results:\")\n",
    "for k in TOP_K:\n",
    "    print(f\"  P@{k}: {cb_results[f'P@{k}']:.4f}\")\n",
    "    print(f\"  R@{k}: {cb_results[f'R@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {cb_results[f'NDCG@{k}']:.4f}\")\n",
    "print(f\"  Coverage: {cb_results['Coverage']:.2f}%\")\n",
    "\n",
    "all_results.append(cb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Hybrid Model (SVD + Content-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6. HYBRID MODEL\n",
      "============================================================\n",
      "Hybrid recommendation function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_hybrid_recommendations(user_idx, predicted_ratings, user_positive, \n",
    "                                item_similarity, user_item_ratings, alpha=0.7, k=20):\n",
    "    \"\"\"\n",
    "    Hybrid recommendations combining SVD and content-based.\n",
    "    \n",
    "    Score = alpha * SVD_score + (1-alpha) * CB_score\n",
    "    \"\"\"\n",
    "    seen = user_positive.get(user_idx, set())\n",
    "    n_items_local = min(predicted_ratings.shape[1], item_similarity.shape[0])\n",
    "    \n",
    "    # SVD scores (normalize to 0-1)\n",
    "    svd_scores = predicted_ratings[user_idx, :n_items_local].copy()\n",
    "    svd_scores = (svd_scores - 0.5) / 4.5  # Normalize from [0.5, 5] to [0, 1]\n",
    "    \n",
    "    # Content-based scores\n",
    "    liked_items = []\n",
    "    for item_idx in seen:\n",
    "        if item_idx in user_item_ratings.get(user_idx, {}):\n",
    "            if user_item_ratings[user_idx][item_idx] >= RELEVANCE_THRESHOLD:\n",
    "                liked_items.append(item_idx)\n",
    "    \n",
    "    if len(liked_items) == 0:\n",
    "        liked_items = [item for item in seen if item < n_items_local][:10]\n",
    "    \n",
    "    cb_scores = np.zeros(n_items_local)\n",
    "    if len(liked_items) > 0:\n",
    "        for liked in liked_items:\n",
    "            if liked < n_items_local:\n",
    "                cb_scores += item_similarity[liked, :n_items_local]\n",
    "        cb_scores /= len(liked_items)\n",
    "    \n",
    "    # Combine\n",
    "    hybrid_scores = alpha * svd_scores + (1 - alpha) * cb_scores\n",
    "    \n",
    "    # Set seen items to -inf\n",
    "    for item in seen:\n",
    "        if item < n_items_local:\n",
    "            hybrid_scores[item] = -np.inf\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "    \n",
    "    return top_k.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning hybrid alpha on validation set...\n",
      "  α=0.3: NDCG@10=0.0274\n",
      "  α=0.5: NDCG@10=0.0366\n",
      "  α=0.7: NDCG@10=0.0330\n",
      "  α=0.9: NDCG@10=0.0271\n",
      "\n",
      "Best alpha: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.3, 0.5, 0.7, 0.9]\n",
    "best_alpha = 0.7\n",
    "best_ndcg = 0.0\n",
    "\n",
    "print(\"\\nTuning hybrid alpha on validation set...\")\n",
    "\n",
    "tune_users = sample_users[:1000]  # Smaller sample for tuning\n",
    "\n",
    "for alpha in alphas:\n",
    "    hybrid_recs_tune = {}\n",
    "    for user_idx in tune_users:\n",
    "        hybrid_recs_tune[user_idx] = get_hybrid_recommendations(\n",
    "            user_idx, predicted_ratings, user_positive_items, \n",
    "            item_similarity, user_item_ratings, alpha=alpha, k=10\n",
    "        )\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_ranking(f'Hybrid(α={alpha})', hybrid_recs_tune, val_relevant, [10])\n",
    "    ndcg = results['NDCG@10']\n",
    "    print(f\"  α={alpha}: NDCG@10={ndcg:.4f}\")\n",
    "    \n",
    "    if ndcg > best_ndcg:\n",
    "        best_ndcg = ndcg\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating hybrid recommendations (α=0.5)...\n",
      "  Processing user 0/3000\n",
      "  Processing user 1000/3000\n",
      "  Processing user 2000/3000\n",
      "\n",
      "Generated recommendations for 3000 users\n"
     ]
    }
   ],
   "source": [
    "# Generate hybrid recommendations with best alpha\n",
    "print(f\"\\nGenerating hybrid recommendations (α={best_alpha})...\")\n",
    "\n",
    "hybrid_recs = {}\n",
    "for i, user_idx in enumerate(sample_users):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processing user {i}/{len(sample_users)}\")\n",
    "    hybrid_recs[user_idx] = get_hybrid_recommendations(\n",
    "        user_idx, predicted_ratings, user_positive_items,\n",
    "        item_similarity, user_item_ratings, alpha=best_alpha, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "print(f\"\\nGenerated recommendations for {len(hybrid_recs)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Results (α=0.5):\n",
      "  P@5: 0.0343\n",
      "  R@5: 0.0167\n",
      "  NDCG@5: 0.0397\n",
      "  P@10: 0.0266\n",
      "  R@10: 0.0271\n",
      "  NDCG@10: 0.0376\n",
      "  P@20: 0.0211\n",
      "  R@20: 0.0404\n",
      "  NDCG@20: 0.0397\n",
      "  Coverage: 5.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate hybrid\n",
    "hybrid_results = evaluate_ranking(f'Hybrid (α={best_alpha})', hybrid_recs, val_relevant, TOP_K)\n",
    "hybrid_results['RMSE'] = '-'\n",
    "hybrid_results['MAE'] = '-'\n",
    "\n",
    "print(f\"\\nHybrid Results (α={best_alpha}):\")\n",
    "for k in TOP_K:\n",
    "    print(f\"  P@{k}: {hybrid_results[f'P@{k}']:.4f}\")\n",
    "    print(f\"  R@{k}: {hybrid_results[f'R@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {hybrid_results[f'NDCG@{k}']:.4f}\")\n",
    "print(f\"  Coverage: {hybrid_results['Coverage']:.2f}%\")\n",
    "\n",
    "all_results.append(hybrid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVD on test set...\n"
     ]
    }
   ],
   "source": [
    "svd_test_recs = {}\n",
    "for user_idx in test_sample_users:\n",
    "    svd_test_recs[user_idx] = get_svd_recommendations(\n",
    "        user_idx, predicted_ratings, user_positive_items, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "svd_test_results = evaluate_ranking('SVD', svd_test_recs, test_relevant, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid on test set...\n"
     ]
    }
   ],
   "source": [
    "hybrid_test_recs = {}\n",
    "for user_idx in test_sample_users:\n",
    "    hybrid_test_recs[user_idx] = get_hybrid_recommendations(\n",
    "        user_idx, predicted_ratings, user_positive_items,\n",
    "        item_similarity, user_item_ratings, alpha=best_alpha, k=max(TOP_K)\n",
    "    )\n",
    "\n",
    "hybrid_test_results = evaluate_ranking(f'Hybrid (α={best_alpha})', hybrid_test_recs, test_relevant, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating test RMSE...\n",
      "  Progress: 0/1,468,027\n",
      "  Progress: 200,000/1,468,027\n",
      "  Progress: 400,000/1,468,027\n",
      "  Progress: 600,000/1,468,027\n",
      "  Progress: 800,000/1,468,027\n",
      "  Progress: 1,000,000/1,468,027\n",
      "  Progress: 1,200,000/1,468,027\n",
      "  Progress: 1,400,000/1,468,027\n",
      "\n",
      "SVD Test Set Rating Prediction:\n",
      "  RMSE: 0.8727\n",
      "  MAE: 0.6576\n"
     ]
    }
   ],
   "source": [
    "test_actuals = test_df['rating'].values\n",
    "\n",
    "test_predictions_svd = []\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    if i % 200000 == 0:\n",
    "        print(f\"  Progress: {i:,}/{len(test_df):,}\")\n",
    "    pred = predict_svd(row['user_idx'], row['item_idx'])\n",
    "    test_predictions_svd.append(pred)\n",
    "\n",
    "rmse_svd_test = rmse(test_predictions_svd, test_actuals)\n",
    "mae_svd_test = mae(test_predictions_svd, test_actuals)\n",
    "\n",
    "print(f\"\\nSVD Test Set Rating Prediction:\")\n",
    "print(f\"  RMSE: {rmse_svd_test:.4f}\")\n",
    "print(f\"  MAE: {mae_svd_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "\n",
      "Model                   NDCG@10       P@10       R@10   Coverage\n",
      "--------------------------------------------------------------\n",
      "SVD                      0.0152     0.0139     0.0077      6.77%\n",
      "Hybrid                   0.0399     0.0284     0.0282      5.67%\n"
     ]
    }
   ],
   "source": [
    "# Print test results comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'NDCG@10':>10} {'P@10':>10} {'R@10':>10} {'Coverage':>10}\")\n",
    "print(\"-\" * 62)\n",
    "print(f\"{'SVD':<20} {svd_test_results['NDCG@10']:>10.4f} {svd_test_results['P@10']:>10.4f} {svd_test_results['R@10']:>10.4f} {svd_test_results['Coverage']:>9.2f}%\")\n",
    "print(f\"{'Hybrid':<20} {hybrid_test_results['NDCG@10']:>10.4f} {hybrid_test_results['P@10']:>10.4f} {hybrid_test_results['R@10']:>10.4f} {hybrid_test_results['Coverage']:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "8. SAVE MODELS AND RESULTS\n",
      "============================================================\n",
      "\n",
      "SVD model saved to D:/Courses/DL INTERNSHIP/THIRD PROJECT/models/svd_model.pkl\n",
      "Item similarity saved to D:/Courses/DL INTERNSHIP/THIRD PROJECT/models/item_similarity.npy\n",
      "Hybrid config saved to D:/Courses/DL INTERNSHIP/THIRD PROJECT/models/hybrid_config.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"8. SAVE MODELS AND RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save SVD components\n",
    "svd_model = {\n",
    "    'user_factors': user_factors,\n",
    "    'item_factors': item_factors,\n",
    "    'global_mean': global_mean,\n",
    "    'user_bias': user_bias,\n",
    "    'item_bias': item_bias,\n",
    "    'n_factors': N_FACTORS\n",
    "}\n",
    "\n",
    "with open(f'{MODELS_PATH}/svd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svd_model, f)\n",
    "print(f\"\\nSVD model saved to {MODELS_PATH}/svd_model.pkl\")\n",
    "\n",
    "# Save item similarity\n",
    "np.save(f'{MODELS_PATH}/item_similarity.npy', item_similarity)\n",
    "print(f\"Item similarity saved to {MODELS_PATH}/item_similarity.npy\")\n",
    "\n",
    "# Save best alpha\n",
    "hybrid_config = {\n",
    "    'best_alpha': best_alpha,\n",
    "    'relevance_threshold': RELEVANCE_THRESHOLD\n",
    "}\n",
    "with open(f'{MODELS_PATH}/hybrid_config.pkl', 'wb') as f:\n",
    "    pickle.dump(hybrid_config, f)\n",
    "print(f\"Hybrid config saved to {MODELS_PATH}/hybrid_config.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "VALIDATION SET RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "         model      RMSE       MAE      P@5     P@10     P@20      R@5     R@10     R@20   NDCG@5  NDCG@10  NDCG@20  Coverage\n",
      "   Global Mean  1.059519  0.838023      NaN 0.000000      NaN      NaN 0.000000      NaN      NaN 0.000000      NaN  0.000000\n",
      "User-Item Bias  0.877511   0.66571      NaN 0.000000      NaN      NaN 0.000000      NaN      NaN 0.000000      NaN  0.000000\n",
      "    Popularity         -         - 0.096741 0.081945 0.065525 0.057444 0.093266 0.146111 0.111190 0.113555 0.124933  0.356390\n",
      "           SVD  0.870696  0.656772 0.011133 0.012133 0.011500 0.003365 0.007294 0.013347 0.012771 0.013742 0.015039  6.771402\n",
      " Content-Based         -         - 0.001733 0.002167 0.002250 0.000820 0.002782 0.005308 0.001718 0.002493 0.003504 35.751691\n",
      "Hybrid (α=0.5)         -         - 0.034267 0.026600 0.021117 0.016665 0.027098 0.040356 0.039655 0.037567 0.039721  5.727689\n",
      "\n",
      "Results saved to D:/Courses/DL INTERNSHIP/THIRD PROJECT/models/validation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create and save results dataframe\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['model', 'RMSE', 'MAE'] + [f'P@{k}' for k in TOP_K] + [f'R@{k}' for k in TOP_K] + [f'NDCG@{k}' for k in TOP_K] + ['Coverage']\n",
    "cols = [c for c in cols if c in results_df.columns]\n",
    "results_df = results_df[cols]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"VALIDATION SET RESULTS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "results_df.to_csv(f'{MODELS_PATH}/validation_results.csv', index=False)\n",
    "print(f\"\\nResults saved to {MODELS_PATH}/validation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 4 SUMMARY: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "MODELS TRAINED\n",
      "----------------------------------------------------------------------\n",
      "  1. Global Mean Baseline\n",
      "  2. User-Item Bias Baseline\n",
      "  3. Popularity Baseline\n",
      "  4. Matrix Factorization (SVD, 50 factors)\n",
      "  5. Content-Based (Genre Similarity)\n",
      "  6. Hybrid (α=0.5)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "BEST MODEL PERFORMANCE (Validation)\n",
      "----------------------------------------------------------------------\n",
      "  Best RMSE: 0.8707 (SVD)\n",
      "  Best NDCG@10: 0.1136 (Popularity)\n",
      "  Best P@10: 0.0819 (Popularity)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "TEST SET PERFORMANCE\n",
      "----------------------------------------------------------------------\n",
      "  SVD RMSE: 0.8727\n",
      "  SVD NDCG@10: 0.0152\n",
      "  Hybrid NDCG@10: 0.0399\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SAVED ARTIFACTS\n",
      "----------------------------------------------------------------------\n",
      "  Location: D:/Courses/DL INTERNSHIP/THIRD PROJECT/models\n",
      "    - hybrid_config.pkl: 0.00 MB\n",
      "    - item_similarity.npy: 2884.45 MB\n",
      "    - svd_model.pkl: 17.75 MB\n",
      "    - validation_results.csv: 0.00 MB\n",
      "\n",
      "======================================================================\n",
      "PHASE 4 COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4 SUMMARY: MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"MODELS TRAINED\")\n",
    "print(\"-\" * 70)\n",
    "print(\"  1. Global Mean Baseline\")\n",
    "print(\"  2. User-Item Bias Baseline\")\n",
    "print(\"  3. Popularity Baseline\")\n",
    "print(f\"  4. Matrix Factorization (SVD, {N_FACTORS} factors)\")\n",
    "print(\"  5. Content-Based (Genre Similarity)\")\n",
    "print(f\"  6. Hybrid (α={best_alpha})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"BEST MODEL PERFORMANCE (Validation)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Find best model for each metric\n",
    "numeric_results = results_df[results_df['RMSE'] != '-'].copy()\n",
    "if len(numeric_results) > 0:\n",
    "    numeric_results['RMSE'] = numeric_results['RMSE'].astype(float)\n",
    "    best_rmse_model = numeric_results.loc[numeric_results['RMSE'].idxmin(), 'model']\n",
    "    best_rmse_val = numeric_results['RMSE'].min()\n",
    "    print(f\"  Best RMSE: {best_rmse_val:.4f} ({best_rmse_model})\")\n",
    "\n",
    "best_ndcg_idx = results_df['NDCG@10'].astype(float).idxmax()\n",
    "best_ndcg_model = results_df.loc[best_ndcg_idx, 'model']\n",
    "best_ndcg_val = results_df.loc[best_ndcg_idx, 'NDCG@10']\n",
    "print(f\"  Best NDCG@10: {best_ndcg_val:.4f} ({best_ndcg_model})\")\n",
    "\n",
    "best_prec_idx = results_df['P@10'].astype(float).idxmax()\n",
    "best_prec_model = results_df.loc[best_prec_idx, 'model']\n",
    "best_prec_val = results_df.loc[best_prec_idx, 'P@10']\n",
    "print(f\"  Best P@10: {best_prec_val:.4f} ({best_prec_model})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  SVD RMSE: {rmse_svd_test:.4f}\")\n",
    "print(f\"  SVD NDCG@10: {svd_test_results['NDCG@10']:.4f}\")\n",
    "print(f\"  Hybrid NDCG@10: {hybrid_test_results['NDCG@10']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"SAVED ARTIFACTS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Location: {MODELS_PATH}\")\n",
    "for f in os.listdir(MODELS_PATH):\n",
    "    size = os.path.getsize(f'{MODELS_PATH}/{f}') / 1024**2\n",
    "    print(f\"    - {f}: {size:.2f} MB\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

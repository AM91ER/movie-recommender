{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: ML Preprocessing (Optimized)\n",
    "## MovieLens 32M Dataset (30% Sample)\n",
    "\n",
    "**Objectives:**\n",
    "1. Create stratified random train/val/test splits\n",
    "2. Ensure all users and items have representation in training\n",
    "3. Build ID mappings and sparse matrices\n",
    "4. Compute statistics from training data only\n",
    "5. Prepare content-based features (genres)\n",
    "6. Save all artifacts for Phase 4 (Model Training)\n",
    "\n",
    "**Why Random Split (not Temporal)?**\n",
    "- Movie preferences are stable over time (genres don't change)\n",
    "- Temporal split causes 93% artificial cold-start\n",
    "- Rating patterns are consistent regardless of when rated\n",
    "- Focus: model quality, not temporal realism\n",
    "\n",
    "**Data Leakage Prevention:**\n",
    "- Statistics computed from training set only\n",
    "- ID mappings fitted on training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION\n",
      "==================================================\n",
      "Input path: D:/Courses/DL INTERNSHIP/THIRD PROJECT/data/processed\n",
      "Output path: D:/Courses/DL INTERNSHIP/THIRD PROJECT/data/ml_ready\n",
      "Split ratios: 0.7/0.15/0.15\n",
      "Min user ratings: 5\n",
      "Min item ratings: 5\n",
      "Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# CONFIGURATION\n",
    "# ===========================================\n",
    "\n",
    "PROCESSED_PATH = 'data/processed'\n",
    "ML_READY_PATH = 'data/ml_ready'\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Minimum ratings thresholds\n",
    "MIN_USER_RATINGS = 5   \n",
    "MIN_ITEM_RATINGS = 5   # Items must have at least this many ratings in training\n",
    "\n",
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(ML_READY_PATH, exist_ok=True)\n",
    "\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input path: {PROCESSED_PATH}\")\n",
    "print(f\"Output path: {ML_READY_PATH}\")\n",
    "print(f\"Split ratios: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"Min user ratings: {MIN_USER_RATINGS}\")\n",
    "print(f\"Min item ratings: {MIN_ITEM_RATINGS}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track all preprocessing steps for summary\n",
    "preprocessing_log = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. LOAD CLEAN DATA\n",
      "============================================================\n",
      "\n",
      "Initial data:\n",
      "  Ratings: 9,659,235\n",
      "  Users: 60,284\n",
      "  Items: 61,455\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. LOAD CLEAN DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ratings_df = pd.read_parquet(f'{PROCESSED_PATH}/ratings_clean.parquet')\n",
    "movies_df = pd.read_parquet(f'{PROCESSED_PATH}/movies_clean.parquet')\n",
    "\n",
    "preprocessing_log['initial_ratings'] = len(ratings_df)\n",
    "preprocessing_log['initial_users'] = ratings_df['userId'].nunique()\n",
    "preprocessing_log['initial_items'] = len(movies_df)\n",
    "\n",
    "print(f\"\\nInitial data:\")\n",
    "print(f\"  Ratings: {preprocessing_log['initial_ratings']:,}\")\n",
    "print(f\"  Users: {preprocessing_log['initial_users']:,}\")\n",
    "print(f\"  Items: {preprocessing_log['initial_items']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Filter Active Users and Items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. FILTER ACTIVE USERS AND ITEMS\n",
      "============================================================\n",
      "\n",
      "Before filtering:\n",
      "  Users with <5 ratings: 0\n",
      "  Items with <5 ratings: 33,951\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"2. FILTER ACTIVE USERS AND ITEMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count ratings per user and item\n",
    "user_counts = ratings_df.groupby('userId').size()\n",
    "item_counts = ratings_df.groupby('movieId').size()\n",
    "\n",
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"  Users with <{MIN_USER_RATINGS} ratings: {(user_counts < MIN_USER_RATINGS).sum():,}\")\n",
    "print(f\"  Items with <{MIN_ITEM_RATINGS} ratings: {(item_counts < MIN_ITEM_RATINGS).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 9,659,235 → 9,596,347 ratings\n",
      "\n",
      "Converged after 2 iterations\n"
     ]
    }
   ],
   "source": [
    "# Iterative filtering (users and items affect each other)\n",
    "for iteration in range(5):  # Usually converges in 2-3 iterations\n",
    "    n_before = len(ratings_df)\n",
    "    \n",
    "    # Filter users\n",
    "    user_counts = ratings_df.groupby('userId').size()\n",
    "    valid_users = user_counts[user_counts >= MIN_USER_RATINGS].index\n",
    "    ratings_df = ratings_df[ratings_df['userId'].isin(valid_users)]\n",
    "    \n",
    "    # Filter items\n",
    "    item_counts = ratings_df.groupby('movieId').size()\n",
    "    valid_items = item_counts[item_counts >= MIN_ITEM_RATINGS].index\n",
    "    ratings_df = ratings_df[ratings_df['movieId'].isin(valid_items)]\n",
    "    \n",
    "    n_after = len(ratings_df)\n",
    "    \n",
    "    if n_before == n_after:\n",
    "        print(f\"\\nConverged after {iteration + 1} iterations\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Iteration {iteration + 1}: {n_before:,} → {n_after:,} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After filtering:\n",
      "  Ratings: 9,596,347 (99.3% kept)\n",
      "  Users: 60,284 (100.0% kept)\n",
      "  Items: 27,504 (44.8% kept)\n"
     ]
    }
   ],
   "source": [
    "preprocessing_log['filtered_ratings'] = len(ratings_df)\n",
    "preprocessing_log['filtered_users'] = ratings_df['userId'].nunique()\n",
    "preprocessing_log['filtered_items'] = ratings_df['movieId'].nunique()\n",
    "\n",
    "print(f\"\\nAfter filtering:\")\n",
    "print(f\"  Ratings: {preprocessing_log['filtered_ratings']:,} ({preprocessing_log['filtered_ratings']/preprocessing_log['initial_ratings']*100:.1f}% kept)\")\n",
    "print(f\"  Users: {preprocessing_log['filtered_users']:,} ({preprocessing_log['filtered_users']/preprocessing_log['initial_users']*100:.1f}% kept)\")\n",
    "print(f\"  Items: {preprocessing_log['filtered_items']:,} ({preprocessing_log['filtered_items']/preprocessing_log['initial_items']*100:.1f}% kept)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. User-Stratified Random Split\n",
    "\n",
    "**Strategy:** For each user, randomly split their ratings into train/val/test.\n",
    "This ensures every user has ratings in all three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. USER-STRATIFIED RANDOM SPLIT\n",
      "============================================================\n",
      "Splitting data by user...\n",
      "\n",
      "Split sizes:\n",
      "  Train: 6,690,428 (69.7%)\n",
      "  Val: 1,437,878 (15.0%)\n",
      "  Test: 1,468,041 (15.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3. USER-STRATIFIED RANDOM SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def user_stratified_split(df, train_ratio=0.7, val_ratio=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Split ratings so each user has ratings in train/val/test.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for user_id, group in df.groupby('userId'):\n",
    "        n = len(group)\n",
    "        indices = np.random.permutation(n)\n",
    "        \n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_idx = indices[:train_end]\n",
    "        val_idx = indices[train_end:val_end]\n",
    "        test_idx = indices[val_end:]\n",
    "        \n",
    "        group_array = group.values\n",
    "        \n",
    "        if len(train_idx) > 0:\n",
    "            train_list.append(group_array[train_idx])\n",
    "        if len(val_idx) > 0:\n",
    "            val_list.append(group_array[val_idx])\n",
    "        if len(test_idx) > 0:\n",
    "            test_list.append(group_array[test_idx])\n",
    "    \n",
    "    columns = df.columns\n",
    "    train_df = pd.DataFrame(np.vstack(train_list), columns=columns)\n",
    "    val_df = pd.DataFrame(np.vstack(val_list), columns=columns)\n",
    "    test_df = pd.DataFrame(np.vstack(test_list), columns=columns)\n",
    "    \n",
    "    # Restore dtypes\n",
    "    for col in ['userId', 'movieId', 'timestamp']:\n",
    "        train_df[col] = train_df[col].astype('int32')\n",
    "        val_df[col] = val_df[col].astype('int32')\n",
    "        test_df[col] = test_df[col].astype('int32')\n",
    "    for col in ['rating']:\n",
    "        train_df[col] = train_df[col].astype('float32')\n",
    "        val_df[col] = val_df[col].astype('float32')\n",
    "        test_df[col] = test_df[col].astype('float32')\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = user_stratified_split(\n",
    "    ratings_df, \n",
    "    train_ratio=TRAIN_RATIO, \n",
    "    val_ratio=VAL_RATIO, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "preprocessing_log['train_size'] = len(train_df)\n",
    "preprocessing_log['val_size'] = len(val_df)\n",
    "preprocessing_log['test_size'] = len(test_df)\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(train_df):,} ({len(train_df)/len(ratings_df)*100:.1f}%)\")\n",
    "print(f\"  Val: {len(val_df):,} ({len(val_df)/len(ratings_df)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_df):,} ({len(test_df)/len(ratings_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User coverage:\n",
      "  Val users in train: 60,284 / 60,284 (100.0%)\n",
      "  Test users in train: 60,284 / 60,284 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Verify user coverage\n",
    "train_users = set(train_df['userId'].unique())\n",
    "val_users = set(val_df['userId'].unique())\n",
    "test_users = set(test_df['userId'].unique())\n",
    "\n",
    "val_users_in_train = len(val_users & train_users)\n",
    "test_users_in_train = len(test_users & train_users)\n",
    "\n",
    "print(f\"\\nUser coverage:\")\n",
    "print(f\"  Val users in train: {val_users_in_train:,} / {len(val_users):,} ({val_users_in_train/len(val_users)*100:.1f}%)\")\n",
    "print(f\"  Test users in train: {test_users_in_train:,} / {len(test_users):,} ({test_users_in_train/len(test_users)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item coverage:\n",
      "  Val items in train: 24,066 / 24,072 (100.0%)\n",
      "  Test items in train: 24,122 / 24,128 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Verify item coverage\n",
    "train_items = set(train_df['movieId'].unique())\n",
    "val_items = set(val_df['movieId'].unique())\n",
    "test_items = set(test_df['movieId'].unique())\n",
    "\n",
    "val_items_in_train = len(val_items & train_items)\n",
    "test_items_in_train = len(test_items & train_items)\n",
    "\n",
    "print(f\"\\nItem coverage:\")\n",
    "print(f\"  Val items in train: {val_items_in_train:,} / {len(val_items):,} ({val_items_in_train/len(val_items)*100:.1f}%)\")\n",
    "print(f\"  Test items in train: {test_items_in_train:,} / {len(test_items):,} ({test_items_in_train/len(test_items)*100:.1f}%)\")\n",
    "\n",
    "preprocessing_log['val_user_coverage'] = val_users_in_train / len(val_users) * 100\n",
    "preprocessing_log['test_user_coverage'] = test_users_in_train / len(test_users) * 100\n",
    "preprocessing_log['val_item_coverage'] = val_items_in_train / len(val_items) * 100\n",
    "preprocessing_log['test_item_coverage'] = test_items_in_train / len(test_items) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ID Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. ID MAPPINGS\n",
      "============================================================\n",
      "\n",
      "Users: 60,284\n",
      "Items: 27,498\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4. ID MAPPINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fit on training data\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_encoder.fit(train_df['userId'])\n",
    "item_encoder.fit(train_df['movieId'])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_items = len(item_encoder.classes_)\n",
    "\n",
    "preprocessing_log['n_users'] = n_users\n",
    "preprocessing_log['n_items'] = n_items\n",
    "\n",
    "print(f\"\\nUsers: {n_users:,}\")\n",
    "print(f\"Items: {n_items:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformation complete.\n"
     ]
    }
   ],
   "source": [
    "# Transform all sets\n",
    "train_df['user_idx'] = user_encoder.transform(train_df['userId'])\n",
    "train_df['item_idx'] = item_encoder.transform(train_df['movieId'])\n",
    "\n",
    "# Safe transform for val/test (handle any edge cases)\n",
    "def safe_transform(encoder, values):\n",
    "    known = set(encoder.classes_)\n",
    "    mask = np.array([v in known for v in values])\n",
    "    result = np.full(len(values), -1, dtype=np.int32)\n",
    "    result[mask] = encoder.transform(values[mask])\n",
    "    return result, mask\n",
    "\n",
    "val_df['user_idx'], val_user_mask = safe_transform(user_encoder, val_df['userId'].values)\n",
    "val_df['item_idx'], val_item_mask = safe_transform(item_encoder, val_df['movieId'].values)\n",
    "\n",
    "test_df['user_idx'], test_user_mask = safe_transform(user_encoder, test_df['userId'].values)\n",
    "test_df['item_idx'], test_item_mask = safe_transform(item_encoder, test_df['movieId'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unknown user/item pairs:\n",
      "  Val: 17 (0.00%)\n",
      "  Test: 14 (0.00%)\n",
      "\n",
      "After filtering unknown:\n",
      "  Val: 1,437,861\n",
      "  Test: 1,468,027\n"
     ]
    }
   ],
   "source": [
    "# Filter val/test to only known users and items\n",
    "val_valid_mask = (val_df['user_idx'] != -1) & (val_df['item_idx'] != -1)\n",
    "test_valid_mask = (test_df['user_idx'] != -1) & (test_df['item_idx'] != -1)\n",
    "\n",
    "val_unknown = (~val_valid_mask).sum()\n",
    "test_unknown = (~test_valid_mask).sum()\n",
    "\n",
    "print(f\"\\nUnknown user/item pairs:\")\n",
    "print(f\"  Val: {val_unknown:,} ({val_unknown/len(val_df)*100:.2f}%)\")\n",
    "print(f\"  Test: {test_unknown:,} ({test_unknown/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "# Filter to keep only valid\n",
    "val_df = val_df[val_valid_mask].copy()\n",
    "test_df = test_df[test_valid_mask].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering unknown:\")\n",
    "print(f\"  Val: {len(val_df):,}\")\n",
    "print(f\"  Test: {len(test_df):,}\")\n",
    "\n",
    "preprocessing_log['val_final'] = len(val_df)\n",
    "preprocessing_log['test_final'] = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compute Statistics (Training Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5. COMPUTE STATISTICS\n",
      "============================================================\n",
      "\n",
      "Global mean: 3.5377\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"5. COMPUTE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Global mean\n",
    "global_mean = train_df['rating'].mean()\n",
    "preprocessing_log['global_mean'] = global_mean\n",
    "print(f\"\\nGlobal mean: {global_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User statistics:\n",
      "  Avg ratings/user: 111.0\n",
      "  Avg user bias: 0.1682\n",
      "  User bias std: 0.4927\n"
     ]
    }
   ],
   "source": [
    "# User statistics\n",
    "user_stats = train_df.groupby('user_idx').agg(\n",
    "    num_ratings=('rating', 'count'),\n",
    "    mean_rating=('rating', 'mean'),\n",
    "    std_rating=('rating', 'std')\n",
    ").reset_index()\n",
    "\n",
    "user_stats['std_rating'] = user_stats['std_rating'].fillna(0)\n",
    "user_stats['bias'] = user_stats['mean_rating'] - global_mean\n",
    "\n",
    "print(f\"\\nUser statistics:\")\n",
    "print(f\"  Avg ratings/user: {user_stats['num_ratings'].mean():.1f}\")\n",
    "print(f\"  Avg user bias: {user_stats['bias'].mean():.4f}\")\n",
    "print(f\"  User bias std: {user_stats['bias'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item statistics:\n",
      "  Avg ratings/item: 243.3\n",
      "  Avg item bias: -0.3827\n",
      "  Item bias std: 0.5748\n"
     ]
    }
   ],
   "source": [
    "# Item statistics\n",
    "item_stats = train_df.groupby('item_idx').agg(\n",
    "    num_ratings=('rating', 'count'),\n",
    "    mean_rating=('rating', 'mean'),\n",
    "    std_rating=('rating', 'std')\n",
    ").reset_index()\n",
    "\n",
    "item_stats['std_rating'] = item_stats['std_rating'].fillna(0)\n",
    "item_stats['bias'] = item_stats['mean_rating'] - global_mean\n",
    "\n",
    "print(f\"\\nItem statistics:\")\n",
    "print(f\"  Avg ratings/item: {item_stats['num_ratings'].mean():.1f}\")\n",
    "print(f\"  Avg item bias: {item_stats['bias'].mean():.4f}\")\n",
    "print(f\"  Item bias std: {item_stats['bias'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lookup dictionaries created.\n"
     ]
    }
   ],
   "source": [
    "# Create lookup dictionaries\n",
    "user_bias_dict = dict(zip(user_stats['user_idx'], user_stats['bias']))\n",
    "item_bias_dict = dict(zip(item_stats['item_idx'], item_stats['bias']))\n",
    "user_mean_dict = dict(zip(user_stats['user_idx'], user_stats['mean_rating']))\n",
    "item_mean_dict = dict(zip(item_stats['item_idx'], item_stats['mean_rating']))\n",
    "\n",
    "# Item popularity for negative sampling\n",
    "item_popularity = dict(zip(item_stats['item_idx'], item_stats['num_ratings']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Create Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6. SPARSE MATRICES\n",
      "============================================================\n",
      "\n",
      "Sparse matrices:\n",
      "  Shape: (60,284, 27,498)\n",
      "  Non-zero: 6,690,428\n",
      "  Density: 0.4036%\n",
      "  Memory (ratings): 25.52 MB\n",
      "  Memory (binary): 51.04 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6. SPARSE MATRICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Rating matrix\n",
    "train_sparse = csr_matrix(\n",
    "    (train_df['rating'].values, (train_df['user_idx'].values, train_df['item_idx'].values)),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Binary interaction matrix\n",
    "train_binary = csr_matrix(\n",
    "    (np.ones(len(train_df)), (train_df['user_idx'].values, train_df['item_idx'].values)),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Density\n",
    "density = train_sparse.nnz / (n_users * n_items) * 100\n",
    "preprocessing_log['density'] = density\n",
    "\n",
    "print(f\"\\nSparse matrices:\")\n",
    "print(f\"  Shape: ({n_users:,}, {n_items:,})\")\n",
    "print(f\"  Non-zero: {train_sparse.nnz:,}\")\n",
    "print(f\"  Density: {density:.4f}%\")\n",
    "print(f\"  Memory (ratings): {train_sparse.data.nbytes / 1024**2:.2f} MB\")\n",
    "print(f\"  Memory (binary): {train_binary.data.nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Content-Based Features (Genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "7. CONTENT-BASED FEATURES\n",
      "============================================================\n",
      "\n",
      "Movies in training: 27,498\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"7. CONTENT-BASED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get movies in training\n",
    "train_movie_ids = set(train_df['movieId'].unique())\n",
    "movies_train = movies_df[movies_df['movieId'].isin(train_movie_ids)].copy()\n",
    "\n",
    "# Add item_idx\n",
    "movies_train['item_idx'] = item_encoder.transform(movies_train['movieId'])\n",
    "\n",
    "print(f\"\\nMovies in training: {len(movies_train):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genres: 19\n",
      "  ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "# Parse and encode genres\n",
    "movies_train['genre_list'] = movies_train['genres'].str.split('|')\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_matrix_raw = mlb.fit_transform(movies_train['genre_list'])\n",
    "genre_names = list(mlb.classes_)\n",
    "\n",
    "preprocessing_log['n_genres'] = len(genre_names)\n",
    "\n",
    "print(f\"\\nGenres: {len(genre_names)}\")\n",
    "print(f\"  {genre_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genre feature matrix:\n",
      "  Shape: (27498, 19)\n",
      "  Items with genres: 27,498 / 27,498\n"
     ]
    }
   ],
   "source": [
    "# Create genre feature matrix aligned with item_idx\n",
    "genre_features = np.zeros((n_items, len(genre_names)), dtype=np.float32)\n",
    "\n",
    "for idx, row in movies_train.iterrows():\n",
    "    item_idx = row['item_idx']\n",
    "    movie_idx_in_df = movies_train.index.get_loc(idx)\n",
    "    genre_features[item_idx] = genre_matrix_raw[movie_idx_in_df]\n",
    "\n",
    "# Verify\n",
    "items_with_genres = (genre_features.sum(axis=1) > 0).sum()\n",
    "print(f\"\\nGenre feature matrix:\")\n",
    "print(f\"  Shape: {genre_features.shape}\")\n",
    "print(f\"  Items with genres: {items_with_genres:,} / {n_items:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "8. EVALUATION DATA\n",
      "============================================================\n",
      "\n",
      "Positive items stored for 60,284 users\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"8. EVALUATION DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Positive items for each user (for negative sampling)\n",
    "user_positive_items = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "all_items = set(range(n_items))\n",
    "\n",
    "print(f\"\\nPositive items stored for {len(user_positive_items):,} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation user groups:\n",
      "  Val: 60,284 users\n",
      "  Test: 60,284 users\n"
     ]
    }
   ],
   "source": [
    "# Group val/test by user for ranking evaluation\n",
    "val_user_items = val_df.groupby('user_idx').apply(\n",
    "    lambda x: list(zip(x['item_idx'].values, x['rating'].values))\n",
    ").to_dict()\n",
    "\n",
    "test_user_items = test_df.groupby('user_idx').apply(\n",
    "    lambda x: list(zip(x['item_idx'].values, x['rating'].values))\n",
    ").to_dict()\n",
    "\n",
    "print(f\"\\nEvaluation user groups:\")\n",
    "print(f\"  Val: {len(val_user_items):,} users\")\n",
    "print(f\"  Test: {len(test_user_items):,} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relevant items (rating >= 4.0):\n",
      "  Val users with relevant: 58,788\n",
      "  Test users with relevant: 59,059\n"
     ]
    }
   ],
   "source": [
    "# Relevant items (rating >= 4.0) for ranking metrics\n",
    "RELEVANCE_THRESHOLD = 4.0\n",
    "\n",
    "val_relevant = val_df[val_df['rating'] >= RELEVANCE_THRESHOLD].groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "test_relevant = test_df[test_df['rating'] >= RELEVANCE_THRESHOLD].groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "\n",
    "print(f\"\\nRelevant items (rating >= {RELEVANCE_THRESHOLD}):\")\n",
    "print(f\"  Val users with relevant: {len(val_relevant):,}\")\n",
    "print(f\"  Test users with relevant: {len(test_relevant):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Create Reverse Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "9. REVERSE MAPPINGS\n",
      "============================================================\n",
      "\n",
      "Reverse mappings created:\n",
      "  idx_to_user: 60,284\n",
      "  idx_to_item: 27,498\n",
      "  item_to_title: 27,498\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"9. REVERSE MAPPINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# idx to original ID\n",
    "idx_to_user = {idx: uid for idx, uid in enumerate(user_encoder.classes_)}\n",
    "idx_to_item = {idx: mid for idx, mid in enumerate(item_encoder.classes_)}\n",
    "\n",
    "# item_idx to movie title\n",
    "item_to_title = dict(zip(movies_train['item_idx'], movies_train['title']))\n",
    "item_to_genres = dict(zip(movies_train['item_idx'], movies_train['genres']))\n",
    "\n",
    "print(f\"\\nReverse mappings created:\")\n",
    "print(f\"  idx_to_user: {len(idx_to_user):,}\")\n",
    "print(f\"  idx_to_item: {len(idx_to_item):,}\")\n",
    "print(f\"  item_to_title: {len(item_to_title):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "10. SAVE ARTIFACTS\n",
      "============================================================\n",
      "\n",
      "1. Split dataframes:\n",
      "   train.parquet: 58.35 MB\n",
      "   val.parquet: 13.96 MB\n",
      "   test.parquet: 14.19 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"10. SAVE ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Split dataframes\n",
    "train_df.to_parquet(f'{ML_READY_PATH}/train.parquet', index=False)\n",
    "val_df.to_parquet(f'{ML_READY_PATH}/val.parquet', index=False)\n",
    "test_df.to_parquet(f'{ML_READY_PATH}/test.parquet', index=False)\n",
    "\n",
    "print(\"\\n1. Split dataframes:\")\n",
    "for f in ['train.parquet', 'val.parquet', 'test.parquet']:\n",
    "    size = os.path.getsize(f'{ML_READY_PATH}/{f}') / 1024**2\n",
    "    print(f\"   {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Sparse matrices:\n",
      "   train_sparse.npz\n",
      "   train_binary.npz\n"
     ]
    }
   ],
   "source": [
    "# 2. Sparse matrices\n",
    "save_npz(f'{ML_READY_PATH}/train_sparse.npz', train_sparse)\n",
    "save_npz(f'{ML_READY_PATH}/train_binary.npz', train_binary)\n",
    "\n",
    "print(\"\\n2. Sparse matrices:\")\n",
    "print(f\"   train_sparse.npz\")\n",
    "print(f\"   train_binary.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Genre features:\n",
      "   genre_features.npy: (27498, 19)\n"
     ]
    }
   ],
   "source": [
    "# 3. Genre features\n",
    "np.save(f'{ML_READY_PATH}/genre_features.npy', genre_features)\n",
    "\n",
    "print(\"\\n3. Genre features:\")\n",
    "print(f\"   genre_features.npy: {genre_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Mappings:\n",
      "   mappings.pkl\n"
     ]
    }
   ],
   "source": [
    "# 4. Mappings\n",
    "mappings = {\n",
    "    'user_encoder': user_encoder,\n",
    "    'item_encoder': item_encoder,\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'idx_to_item': idx_to_item,\n",
    "    'item_to_title': item_to_title,\n",
    "    'item_to_genres': item_to_genres,\n",
    "    'genre_encoder': mlb,\n",
    "    'genre_names': genre_names\n",
    "}\n",
    "\n",
    "with open(f'{ML_READY_PATH}/mappings.pkl', 'wb') as f:\n",
    "    pickle.dump(mappings, f)\n",
    "\n",
    "print(\"\\n4. Mappings:\")\n",
    "print(f\"   mappings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Statistics:\n",
      "   stats.pkl\n"
     ]
    }
   ],
   "source": [
    "# 5. Statistics\n",
    "stats = {\n",
    "    'global_mean': global_mean,\n",
    "    'user_bias': user_bias_dict,\n",
    "    'item_bias': item_bias_dict,\n",
    "    'user_mean': user_mean_dict,\n",
    "    'item_mean': item_mean_dict,\n",
    "    'item_popularity': item_popularity\n",
    "}\n",
    "\n",
    "with open(f'{ML_READY_PATH}/stats.pkl', 'wb') as f:\n",
    "    pickle.dump(stats, f)\n",
    "\n",
    "print(\"\\n5. Statistics:\")\n",
    "print(f\"   stats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Evaluation data:\n",
      "   eval_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation data\n",
    "eval_data = {\n",
    "    'val_user_items': val_user_items,\n",
    "    'test_user_items': test_user_items,\n",
    "    'val_relevant': val_relevant,\n",
    "    'test_relevant': test_relevant,\n",
    "    'user_positive_items': user_positive_items,\n",
    "    'all_items': all_items,\n",
    "    'relevance_threshold': RELEVANCE_THRESHOLD\n",
    "}\n",
    "\n",
    "with open(f'{ML_READY_PATH}/eval_data.pkl', 'wb') as f:\n",
    "    pickle.dump(eval_data, f)\n",
    "\n",
    "print(\"\\n6. Evaluation data:\")\n",
    "print(f\"   eval_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Movies metadata:\n",
      "   movies_train.parquet\n"
     ]
    }
   ],
   "source": [
    "# 7. Movies metadata\n",
    "movies_train.to_parquet(f'{ML_READY_PATH}/movies_train.parquet', index=False)\n",
    "\n",
    "print(\"\\n7. Movies metadata:\")\n",
    "print(f\"   movies_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Config:\n",
      "   config.pkl\n"
     ]
    }
   ],
   "source": [
    "# 8. Preprocessing config (for reproducibility)\n",
    "config = {\n",
    "    'train_ratio': TRAIN_RATIO,\n",
    "    'val_ratio': VAL_RATIO,\n",
    "    'test_ratio': TEST_RATIO,\n",
    "    'min_user_ratings': MIN_USER_RATINGS,\n",
    "    'min_item_ratings': MIN_ITEM_RATINGS,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'relevance_threshold': RELEVANCE_THRESHOLD,\n",
    "    'split_type': 'user_stratified_random'\n",
    "}\n",
    "\n",
    "with open(f'{ML_READY_PATH}/config.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(\"\\n8. Config:\")\n",
    "print(f\"   config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 3 SUMMARY: ML PREPROCESSING (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "1. DATA FILTERING\n",
      "----------------------------------------------------------------------\n",
      "   Initial: 9,659,235 ratings, 60,284 users, 61,455 items\n",
      "   Filtered: 9,596,347 ratings, 60,284 users, 27,504 items\n",
      "   Kept: 99.3% of ratings\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "2. USER-STRATIFIED RANDOM SPLIT\n",
      "----------------------------------------------------------------------\n",
      "   Train: 6,690,428 ratings (70%)\n",
      "   Val: 1,437,861 ratings\n",
      "   Test: 1,468,027 ratings\n",
      "   ✓ Every user has ratings in all splits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "3. COVERAGE\n",
      "----------------------------------------------------------------------\n",
      "   Val users in train: 100.0%\n",
      "   Val items in train: 100.0%\n",
      "   Test users in train: 100.0%\n",
      "   Test items in train: 100.0%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "4. MATRIX\n",
      "----------------------------------------------------------------------\n",
      "   Shape: (60,284, 27,498)\n",
      "   Density: 0.4036%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "5. STATISTICS\n",
      "----------------------------------------------------------------------\n",
      "   Global mean: 3.5377\n",
      "   User biases: 60,284\n",
      "   Item biases: 27,498\n",
      "   ✓ Computed from training only\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "6. CONTENT FEATURES\n",
      "----------------------------------------------------------------------\n",
      "   Genres: 19\n",
      "   Feature matrix: (27,498, 19)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "7. OUTPUT FILES\n",
      "----------------------------------------------------------------------\n",
      "   Location: D:/Courses/DL INTERNSHIP/THIRD PROJECT/data/ml_ready\n",
      "     - config.pkl: 0.00 MB\n",
      "     - eval_data.pkl: 113.59 MB\n",
      "     - genre_features.npy: 1.99 MB\n",
      "     - mappings.pkl: 2.93 MB\n",
      "     - movies_train.parquet: 1.02 MB\n",
      "     - stats.pkl: 2.14 MB\n",
      "     - test.parquet: 14.19 MB\n",
      "     - test_cf.parquet: 1.14 MB\n",
      "     - train.parquet: 58.35 MB\n",
      "     - train_binary.npz: 11.11 MB\n",
      "     - train_sparse.npz: 14.74 MB\n",
      "     - val.parquet: 13.96 MB\n",
      "     - val_cf.parquet: 2.12 MB\n",
      "   Total: 237.30 MB\n",
      "\n",
      "======================================================================\n",
      "IMPROVEMENTS vs TEMPORAL SPLIT\n",
      "======================================================================\n",
      "   ✓ User coverage: ~100% (was ~7% with temporal)\n",
      "   ✓ Item coverage: ~100%+ (was ~7% with temporal)\n",
      "   ✓ No artificial cold-start problem\n",
      "   ✓ All val/test ratings usable for CF evaluation\n",
      "\n",
      "======================================================================\n",
      "READY FOR PHASE 4: MODEL TRAINING\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3 SUMMARY: ML PREPROCESSING (OPTIMIZED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"1. DATA FILTERING\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Initial: {preprocessing_log['initial_ratings']:,} ratings, {preprocessing_log['initial_users']:,} users, {preprocessing_log['initial_items']:,} items\")\n",
    "print(f\"   Filtered: {preprocessing_log['filtered_ratings']:,} ratings, {preprocessing_log['filtered_users']:,} users, {preprocessing_log['filtered_items']:,} items\")\n",
    "print(f\"   Kept: {preprocessing_log['filtered_ratings']/preprocessing_log['initial_ratings']*100:.1f}% of ratings\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"2. USER-STRATIFIED RANDOM SPLIT\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Train: {preprocessing_log['train_size']:,} ratings ({TRAIN_RATIO*100:.0f}%)\")\n",
    "print(f\"   Val: {preprocessing_log['val_final']:,} ratings\")\n",
    "print(f\"   Test: {preprocessing_log['test_final']:,} ratings\")\n",
    "print(f\"   ✓ Every user has ratings in all splits\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"3. COVERAGE\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Val users in train: {preprocessing_log['val_user_coverage']:.1f}%\")\n",
    "print(f\"   Val items in train: {preprocessing_log['val_item_coverage']:.1f}%\")\n",
    "print(f\"   Test users in train: {preprocessing_log['test_user_coverage']:.1f}%\")\n",
    "print(f\"   Test items in train: {preprocessing_log['test_item_coverage']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"4. MATRIX\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Shape: ({preprocessing_log['n_users']:,}, {preprocessing_log['n_items']:,})\")\n",
    "print(f\"   Density: {preprocessing_log['density']:.4f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"5. STATISTICS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Global mean: {preprocessing_log['global_mean']:.4f}\")\n",
    "print(f\"   User biases: {len(user_bias_dict):,}\")\n",
    "print(f\"   Item biases: {len(item_bias_dict):,}\")\n",
    "print(f\"   ✓ Computed from training only\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"6. CONTENT FEATURES\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Genres: {preprocessing_log['n_genres']}\")\n",
    "print(f\"   Feature matrix: ({preprocessing_log['n_items']:,}, {preprocessing_log['n_genres']})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"7. OUTPUT FILES\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Location: {ML_READY_PATH}\")\n",
    "total_size = 0\n",
    "for f in os.listdir(ML_READY_PATH):\n",
    "    size = os.path.getsize(f'{ML_READY_PATH}/{f}') / 1024**2\n",
    "    total_size += size\n",
    "    print(f\"     - {f}: {size:.2f} MB\")\n",
    "print(f\"   Total: {total_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMPROVEMENTS vs TEMPORAL SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   ✓ User coverage: ~100% (was ~7% with temporal)\")\n",
    "print(f\"   ✓ Item coverage: ~{preprocessing_log['val_item_coverage']:.0f}%+ (was ~7% with temporal)\")\n",
    "print(f\"   ✓ No artificial cold-start problem\")\n",
    "print(f\"   ✓ All val/test ratings usable for CF evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"READY FOR PHASE 4: MODEL TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
